{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a968bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc72df4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f48850582b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc07c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34376698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "# 2 in local means the number of cores allocated\n",
    "# setMaster() used to set Spark Context Manager which is local [core_of_cpu]\n",
    "config =  SparkConf().setMaster(\"local[4]\").setAppName(\"ETL Session\")\n",
    "\n",
    "sc = SparkContext(conf = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1f43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ETL Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e13de1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f48850226a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9fa60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=ETL Session>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721a10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF = spark.read.format('jdbc')\\\n",
    "    .option('url', 'jdbc:mysql://localhost:3306/hremployeedb')\\\n",
    "    .option('dbtable', 'HR_Employee')\\\n",
    "    .option('user', 'root')\\\n",
    "    .option('password', 'hadoop@123')\\\n",
    "    .option('driver', 'com.mysql.cj.jdbc.Driver').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06dbc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|EmployeeID|          Department|             JobRole|Attrition|Gender|Age|MaritalStatus|    Education|EducationField|   BusinessTravel|JobInvolvement|JobLevel|JobSatisfaction|Hourlyrate|Income|Salaryhike|OverTime|Workex|YearsSinceLastPromotion|EmpSatisfaction|TrainingTimesLastYear|WorkLifeBalance|Performance_Rating|\n",
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|         1|               Sales|     Sales Executive|      Yes|Female| 41|       Single|      College| Life Sciences|    Travel_Rarely|          High|       2|      Very High|        94|  5993|        11|     Yes|     8|                      0|         Medium|                    0|            Bad|        Excellent\r",
      "|\n",
      "|         2|Research & Develo...|  Research Scientist|       No|  Male| 49|      Married|Below College| Life Sciences|Travel_Frequently|        Medium|       2|         Medium|        61|  5130|        23|      No|    10|                      1|           High|                    3|         Better|      Outstanding\r",
      "|\n",
      "|         3|Research & Develo...|Laboratory Techni...|      Yes|  Male| 37|       Single|      College|         Other|    Travel_Rarely|        Medium|       1|           High|        92|  2090|        15|     Yes|     7|                      0|      Very High|                    3|         Better|        Excellent\r",
      "|\n",
      "|         4|Research & Develo...|  Research Scientist|       No|Female| 33|      Married|       Master| Life Sciences|Travel_Frequently|          High|       1|           High|        56|  2909|        11|     Yes|     8|                      3|      Very High|                    3|         Better|        Excellent\r",
      "|\n",
      "|         5|Research & Develo...|Laboratory Techni...|       No|  Male| 27|      Married|Below College|       Medical|    Travel_Rarely|          High|       1|         Medium|        40|  3468|        12|      No|     6|                      2|            Low|                    3|         Better|        Excellent\r",
      "|\n",
      "|         6|Research & Develo...|Laboratory Techni...|       No|  Male| 32|       Single|      College| Life Sciences|Travel_Frequently|          High|       1|      Very High|        79|  3068|        13|      No|     8|                      3|      Very High|                    2|           Good|        Excellent\r",
      "|\n",
      "|         7|Research & Develo...|Laboratory Techni...|       No|Female| 59|      Married|     Bachelor|       Medical|    Travel_Rarely|     Very High|       1|            Low|        81|  2670|        20|     Yes|    12|                      0|           High|                    3|           Good|      Outstanding\r",
      "|\n",
      "|         8|Research & Develo...|Laboratory Techni...|       No|  Male| 30|     Divorced|Below College| Life Sciences|    Travel_Rarely|          High|       1|           High|        67|  2693|        22|      No|     1|                      0|      Very High|                    2|         Better|      Outstanding\r",
      "|\n",
      "|         9|Research & Develo...|Manufacturing Dir...|       No|  Male| 38|       Single|     Bachelor| Life Sciences|Travel_Frequently|        Medium|       3|           High|        44|  9526|        21|      No|    10|                      1|      Very High|                    2|         Better|      Outstanding\r",
      "|\n",
      "|        10|Research & Develo...|Healthcare Repres...|       No|  Male| 36|      Married|     Bachelor|       Medical|    Travel_Rarely|          High|       2|           High|        94|  5237|        13|      No|    17|                      7|           High|                    3|           Good|        Excellent\r",
      "|\n",
      "|        11|Research & Develo...|Laboratory Techni...|       No|  Male| 35|      Married|     Bachelor|       Medical|    Travel_Rarely|     Very High|       1|         Medium|        84|  2426|        13|      No|     6|                      0|            Low|                    5|         Better|        Excellent\r",
      "|\n",
      "|        12|Research & Develo...|Laboratory Techni...|       No|Female| 29|       Single|      College| Life Sciences|    Travel_Rarely|        Medium|       2|           High|        49|  4193|        12|     Yes|    10|                      0|      Very High|                    3|         Better|        Excellent\r",
      "|\n",
      "|        13|Research & Develo...|  Research Scientist|       No|  Male| 31|     Divorced|Below College| Life Sciences|    Travel_Rarely|          High|       1|           High|        31|  2911|        17|      No|     5|                      4|            Low|                    1|           Good|        Excellent\r",
      "|\n",
      "|        14|Research & Develo...|Laboratory Techni...|       No|  Male| 34|     Divorced|      College|       Medical|    Travel_Rarely|          High|       1|      Very High|        93|  2661|        11|      No|     3|                      1|         Medium|                    2|         Better|        Excellent\r",
      "|\n",
      "|        15|Research & Develo...|Laboratory Techni...|      Yes|  Male| 28|       Single|     Bachelor| Life Sciences|    Travel_Rarely|        Medium|       1|           High|        50|  2028|        14|     Yes|     6|                      0|           High|                    4|         Better|        Excellent\r",
      "|\n",
      "|        16|Research & Develo...|Manufacturing Dir...|       No|Female| 29|     Divorced|       Master| Life Sciences|    Travel_Rarely|     Very High|       3|            Low|        51|  9980|        11|      No|    10|                      8|         Medium|                    1|         Better|        Excellent\r",
      "|\n",
      "|        17|Research & Develo...|  Research Scientist|       No|  Male| 32|     Divorced|      College| Life Sciences|    Travel_Rarely|     Very High|       1|         Medium|        80|  3298|        12|     Yes|     7|                      0|            Low|                    5|           Good|        Excellent\r",
      "|\n",
      "|        18|Research & Develo...|Laboratory Techni...|       No|  Male| 22|     Divorced|      College|       Medical|       Non-Travel|     Very High|       1|      Very High|        96|  2935|        13|     Yes|     1|                      0|      Very High|                    2|           Good|        Excellent\r",
      "|\n",
      "|        19|               Sales|             Manager|       No|Female| 53|      Married|       Master| Life Sciences|    Travel_Rarely|        Medium|       4|      Very High|        78| 15427|        16|      No|    31|                      3|            Low|                    3|         Better|        Excellent\r",
      "|\n",
      "|        20|Research & Develo...|  Research Scientist|       No|  Male| 38|       Single|     Bachelor| Life Sciences|    Travel_Rarely|          High|       1|      Very High|        45|  3944|        11|     Yes|     6|                      1|      Very High|                    3|         Better|        Excellent\r",
      "|\n",
      "+----------+--------------------+--------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hremployeeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2d9ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(HR_Employee) [numPartitions=1] [EmployeeID#0,Department#1,JobRole#2,Attrition#3,Gender#4,Age#5,MaritalStatus#6,Education#7,EducationField#8,BusinessTravel#9,JobInvolvement#10,JobLevel#11,JobSatisfaction#12,Hourlyrate#13,Income#14,Salaryhike#15,OverTime#16,Workex#17,YearsSinceLastPromotion#18,EmpSatisfaction#19,TrainingTimesLastYear#20,WorkLifeBalance#21,Performance_Rating#22] PushedFilters: [], ReadSchema: struct<EmployeeID:int,Department:string,JobRole:string,Attrition:string,Gender:string,Age:int,Mar...\n"
     ]
    }
   ],
   "source": [
    "# Show Physical plan of execution which is known as DAG.\n",
    "hremployeeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295add8",
   "metadata": {},
   "source": [
    "#### Materialized View of Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf3f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF.createOrReplaceTempView(\"hremployee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1bb374",
   "metadata": {},
   "source": [
    "#### 1. Display shape of hremployee table\n",
    "        * Show number of rows & number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0cd719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|ROW_COUNT|\n",
      "+---------+\n",
      "|     1469|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS ROW_COUNT\n",
    "    FROM hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58d38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cols = len(hremployeeDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e396c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hremployeeDF.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1071ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|ROW_COUNT|COLUMN_COUNT|\n",
      "+---------+------------+\n",
      "|     1469|          23|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS ROW_COUNT,\n",
    "    {num_of_cols} AS COLUMN_COUNT\n",
    "    FROM hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fc1b6",
   "metadata": {},
   "source": [
    "#### MySql query to find the shape of the database\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS ROW_COUNT,\n",
    "    SIZE(COLLECT_LIST(*)) AS COLUMN_COUNT\n",
    "    FROM hremployee\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5d2016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EmployeeID',\n",
       " 'Department',\n",
       " 'JobRole',\n",
       " 'Attrition',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'MaritalStatus',\n",
       " 'Education',\n",
       " 'EducationField',\n",
       " 'BusinessTravel',\n",
       " 'JobInvolvement',\n",
       " 'JobLevel',\n",
       " 'JobSatisfaction',\n",
       " 'Hourlyrate',\n",
       " 'Income',\n",
       " 'Salaryhike',\n",
       " 'OverTime',\n",
       " 'Workex',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'EmpSatisfaction',\n",
       " 'TrainingTimesLastYear',\n",
       " 'WorkLifeBalance',\n",
       " 'Performance_Rating']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hremployeeDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b7430",
   "metadata": {},
   "source": [
    "#### 2. Write a Query to show first 3 employee from each job role to join the company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dc7648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|EmployeeID|             JobRole|\n",
      "+----------+--------------------+\n",
      "|         1|     Sales Executive|\n",
      "|        28|     Sales Executive|\n",
      "|        40|     Sales Executive|\n",
      "|         9|Manufacturing Dir...|\n",
      "|        16|Manufacturing Dir...|\n",
      "|        21|Manufacturing Dir...|\n",
      "|         3|Laboratory Techni...|\n",
      "|         5|Laboratory Techni...|\n",
      "|         6|Laboratory Techni...|\n",
      "|        22|Sales Representative|\n",
      "|        34|Sales Representative|\n",
      "|        37|Sales Representative|\n",
      "|        10|Healthcare Repres...|\n",
      "|        29|Healthcare Repres...|\n",
      "|        32|Healthcare Repres...|\n",
      "|         2|  Research Scientist|\n",
      "|         4|  Research Scientist|\n",
      "|        13|  Research Scientist|\n",
      "|        19|             Manager|\n",
      "|        26|             Manager|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT EmployeeID,JobRole\n",
    "    FROM (\n",
    "        SELECT EmployeeID,JobRole,\n",
    "        row_number() OVER(partition by JobRole ORDER BY EmployeeID)\n",
    "        AS rank\n",
    "        FROM hremployee\n",
    "        ) AS _\n",
    "    WHERE rank < 4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00b201",
   "metadata": {},
   "source": [
    "#### 3. Top 3 employee with highest salary in Job Role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29b5e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+\n",
      "|EmployeeID|             JobRole|Income|\n",
      "+----------+--------------------+------+\n",
      "|        99|     Sales Executive| 13872|\n",
      "|       545|     Sales Executive| 13770|\n",
      "|       839|     Sales Executive| 13758|\n",
      "|       722|Manufacturing Dir...| 13973|\n",
      "|       628|Manufacturing Dir...| 13826|\n",
      "|       744|Manufacturing Dir...| 13726|\n",
      "|       678|Laboratory Techni...|  7403|\n",
      "|       817|Laboratory Techni...|  6782|\n",
      "|       945|Laboratory Techni...|  6674|\n",
      "|       565|Sales Representative|  6632|\n",
      "|      1308|Sales Representative|  5405|\n",
      "|      1220|Sales Representative|  4502|\n",
      "|      1181|Healthcare Repres...| 13966|\n",
      "|       317|Healthcare Repres...| 13964|\n",
      "|       190|Healthcare Repres...| 13734|\n",
      "|        68|  Research Scientist|  9724|\n",
      "|      1315|  Research Scientist|  6962|\n",
      "|      1305|  Research Scientist|  6854|\n",
      "|       191|             Manager| 19999|\n",
      "|       852|             Manager| 19943|\n",
      "|       166|             Manager| 19926|\n",
      "+----------+--------------------+------+\n",
      "only showing top 21 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT EmployeeID,JobRole,Income\n",
    "    FROM (\n",
    "        SELECT EmployeeID,JobRole,Income,\n",
    "        row_number() OVER(partition by JobRole ORDER BY Income DESC)\n",
    "        AS rank\n",
    "        FROM hremployee\n",
    "        ) AS _\n",
    "    WHERE rank < 4\n",
    "\"\"\").show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c536d4",
   "metadata": {},
   "source": [
    "#### Top 3 highest package from overall Job Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "808ec33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------+\n",
      "|EmployeeID|          JobRole|Income|\n",
      "+----------+-----------------+------+\n",
      "|       191|          Manager| 19999|\n",
      "|       747|Research Director| 19973|\n",
      "|       852|          Manager| 19943|\n",
      "+----------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT EmployeeID,JobRole,Income\n",
    "    FROM (\n",
    "        SELECT EmployeeID,JobRole,Income,\n",
    "        row_number() OVER(ORDER BY Income DESC)\n",
    "        AS rank\n",
    "        FROM hremployee\n",
    "        ) AS _\n",
    "    ORDER BY rank\n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52debce",
   "metadata": {},
   "source": [
    "#### write a spark query to shoq employee in order of Ascending with respect to employee income compared to previous income for each job role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e1fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+--------+-----+\n",
      "|EmployeeID|             JobRole|Income|prev_inc| diff|\n",
      "+----------+--------------------+------+--------+-----+\n",
      "|        10|Healthcare Repres...|  5237|    null| null|\n",
      "|       285|Healthcare Repres...|  4741|   13496|-8755|\n",
      "|      1183|Healthcare Repres...|  6842|   13966|-7124|\n",
      "|      1157|Healthcare Repres...|  4148|   11245|-7097|\n",
      "|       205|Healthcare Repres...|  6673|   13734|-7061|\n",
      "|       677|Healthcare Repres...|  4014|   10552|-6538|\n",
      "|       397|Healthcare Repres...|  4522|   10965|-6443|\n",
      "|       833|Healthcare Repres...|  5731|   12169|-6438|\n",
      "|      1065|Healthcare Repres...|  4035|   10466|-6431|\n",
      "|       745|Healthcare Repres...|  4777|   10999|-6222|\n",
      "|       736|Healthcare Repres...|  4240|   10388|-6148|\n",
      "|      1098|Healthcare Repres...|  4069|   10124|-6055|\n",
      "|        89|Healthcare Repres...|  4152|   10096|-5944|\n",
      "|       489|Healthcare Repres...|  4089|    9824|-5735|\n",
      "|       929|Healthcare Repres...|  7978|   13577|-5599|\n",
      "|       105|Healthcare Repres...|  5163|   10673|-5510|\n",
      "|       267|Healthcare Repres...|  5582|   10938|-5356|\n",
      "|      1231|Healthcare Repres...|  5562|   10748|-5186|\n",
      "|       555|Healthcare Repres...|  6811|   11103|-4292|\n",
      "|      1045|Healthcare Repres...|  6651|   10851|-4200|\n",
      "|       527|Healthcare Repres...|  4553|    8621|-4068|\n",
      "|       334|Healthcare Repres...|  9985|   13964|-3979|\n",
      "|       373|Healthcare Repres...|  6540|   10496|-3956|\n",
      "|       309|Healthcare Repres...|  5660|    9613|-3953|\n",
      "|      1131|Healthcare Repres...|  5063|    8853|-3790|\n",
      "|        32|Healthcare Repres...|  6465|   10248|-3783|\n",
      "|      1425|Healthcare Repres...|  4878|    8633|-3755|\n",
      "|      1008|Healthcare Repres...|  7553|   10920|-3367|\n",
      "|       357|Healthcare Repres...|  6781|    9985|-3204|\n",
      "|       791|Healthcare Repres...|  7119|   10322|-3203|\n",
      "|       460|Healthcare Repres...|  6811|    9824|-3013|\n",
      "|      1214|Healthcare Repres...|  7879|   10883|-3004|\n",
      "|      1407|Healthcare Repres...|  4617|    7510|-2893|\n",
      "|        94|Healthcare Repres...| 10673|   13503|-2830|\n",
      "|      1287|Healthcare Repres...|  5538|    8321|-2783|\n",
      "|      1299|Healthcare Repres...|  6513|    8926|-2413|\n",
      "|       768|Healthcare Repres...|  4107|    6385|-2278|\n",
      "|       979|Healthcare Repres...|  6377|    8500|-2123|\n",
      "|      1304|Healthcare Repres...|  4448|    6513|-2065|\n",
      "|       209|Healthcare Repres...|  4876|    6673|-1797|\n",
      "|       558|Healthcare Repres...|  5093|    6811|-1718|\n",
      "|       268|Healthcare Repres...|  4000|    5582|-1582|\n",
      "|      1199|Healthcare Repres...|  5347|    6842|-1495|\n",
      "|       550|Healthcare Repres...|  6142|    7625|-1483|\n",
      "|       716|Healthcare Repres...|  5488|    6949|-1461|\n",
      "|      1352|Healthcare Repres...|  5033|    6384|-1351|\n",
      "|      1123|Healthcare Repres...|  6142|    7441|-1299|\n",
      "|      1259|Healthcare Repres...|  5294|    6294|-1000|\n",
      "|       943|Healthcare Repres...|  7094|    7978| -884|\n",
      "|       164|Healthcare Repres...|  9439|   10312| -873|\n",
      "|       573|Healthcare Repres...|  4335|    5093| -758|\n",
      "|       472|Healthcare Repres...|  9824|   10527| -703|\n",
      "|      1398|Healthcare Repres...|  5968|    6667| -699|\n",
      "|      1456|Healthcare Repres...|  5689|    6306| -617|\n",
      "|       848|Healthcare Repres...|  5343|    5731| -388|\n",
      "|       651|Healthcare Repres...|  5562|    5933| -371|\n",
      "|       618|Healthcare Repres...|  5933|    6288| -355|\n",
      "|       904|Healthcare Repres...|  6623|    6812| -189|\n",
      "|       583|Healthcare Repres...|  4244|    4335|  -91|\n",
      "|       303|Healthcare Repres...|  5661|    5745|  -84|\n",
      "|       361|Healthcare Repres...|  6755|    6781|  -26|\n",
      "|      1387|Healthcare Repres...|  5373|    5399|  -26|\n",
      "|       897|Healthcare Repres...|  6812|    6833|  -21|\n",
      "|       414|Healthcare Repres...|  4523|    4522|    1|\n",
      "|       785|Healthcare Repres...|  8823|    8722|  101|\n",
      "|        65|Healthcare Repres...| 10096|    9884|  212|\n",
      "|       984|Healthcare Repres...|  6687|    6377|  310|\n",
      "|      1362|Healthcare Repres...|  5399|    5033|  366|\n",
      "|      1093|Healthcare Repres...| 10124|    9715|  409|\n",
      "|      1261|Healthcare Repres...|  5811|    5294|  517|\n",
      "|       795|Healthcare Repres...|  7756|    7119|  637|\n",
      "|      1250|Healthcare Repres...|  6294|    5562|  732|\n",
      "|       525|Healthcare Repres...|  8621|    7725|  896|\n",
      "|       288|Healthcare Repres...|  5745|    4741| 1004|\n",
      "|       665|Healthcare Repres...|  6586|    5562| 1024|\n",
      "|       695|Healthcare Repres...|  6949|    5855| 1094|\n",
      "|      1090|Healthcare Repres...|  9715|    8606| 1109|\n",
      "|      1388|Healthcare Repres...|  6667|    5373| 1294|\n",
      "|      1294|Healthcare Repres...|  6870|    5538| 1332|\n",
      "|       959|Healthcare Repres...|  8500|    7094| 1406|\n",
      "|      1441|Healthcare Repres...|  6306|    4878| 1428|\n",
      "|       896|Healthcare Repres...|  6833|    5343| 1490|\n",
      "|       786|Healthcare Repres...| 10322|    8823| 1499|\n",
      "|      1399|Healthcare Repres...|  7510|    5968| 1542|\n",
      "|       252|Healthcare Repres...| 10938|    9396| 1542|\n",
      "|       746|Healthcare Repres...|  6385|    4777| 1608|\n",
      "|      1203|Healthcare Repres...|  7005|    5347| 1658|\n",
      "|       814|Healthcare Repres...| 12169|   10445| 1724|\n",
      "|       691|Healthcare Repres...|  5855|    4014| 1841|\n",
      "|      1324|Healthcare Repres...|  6384|    4448| 1936|\n",
      "|       606|Healthcare Repres...|  6288|    4244| 2044|\n",
      "|      1298|Healthcare Repres...|  8926|    6870| 2056|\n",
      "|       440|Healthcare Repres...|  9824|    7632| 2192|\n",
      "|      1081|Healthcare Repres...|  8606|    6388| 2218|\n",
      "|       111|Healthcare Repres...|  7484|    5163| 2321|\n",
      "|      1074|Healthcare Repres...|  6388|    4035| 2353|\n",
      "|      1278|Healthcare Repres...|  8321|    5811| 2510|\n",
      "|       807|Healthcare Repres...| 10445|    7756| 2689|\n",
      "|      1125|Healthcare Repres...|  8853|    6142| 2711|\n",
      "|       127|Healthcare Repres...| 10312|    7484| 2828|\n",
      "+----------+--------------------+------+--------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT EmployeeID,JobRole,Income,\n",
    "        lag(Income, 1) OVER(PARTITION BY JobRole ORDER BY EmployeeID) prev_inc,\n",
    "        Income - lag(Income, 1) OVER(PARTITION BY JobRole ORDER BY EmployeeID)\n",
    "        AS diff\n",
    "        FROM hremployee\n",
    "        ORDER BY JobRole,diff\n",
    "\"\"\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60475495",
   "metadata": {},
   "source": [
    "#### LEAD() -> Row's Next Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e2ceaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+------+------+------+--------+\n",
      "|EmployeeID|          Department|             jobRole|age|gender|Income|Workex|next_inc|\n",
      "+----------+--------------------+--------------------+---+------+------+------+--------+\n",
      "|         1|               Sales|     Sales Executive| 41|Female|  5993|     8|    2090|\n",
      "|         2|Research & Develo...|  Research Scientist| 49|  Male|  5130|    10|    2909|\n",
      "|         3|Research & Develo...|Laboratory Techni...| 37|  Male|  2090|     7|    3468|\n",
      "|         4|Research & Develo...|  Research Scientist| 33|Female|  2909|     8|    3068|\n",
      "|         5|Research & Develo...|Laboratory Techni...| 27|  Male|  3468|     6|    2670|\n",
      "|         6|Research & Develo...|Laboratory Techni...| 32|  Male|  3068|     8|    2693|\n",
      "|         7|Research & Develo...|Laboratory Techni...| 59|Female|  2670|    12|    9526|\n",
      "|         8|Research & Develo...|Laboratory Techni...| 30|  Male|  2693|     1|    5237|\n",
      "|         9|Research & Develo...|Manufacturing Dir...| 38|  Male|  9526|    10|    2426|\n",
      "|        10|Research & Develo...|Healthcare Repres...| 36|  Male|  5237|    17|    4193|\n",
      "|        11|Research & Develo...|Laboratory Techni...| 35|  Male|  2426|     6|    2911|\n",
      "|        12|Research & Develo...|Laboratory Techni...| 29|Female|  4193|    10|    2661|\n",
      "|        13|Research & Develo...|  Research Scientist| 31|  Male|  2911|     5|    2028|\n",
      "|        14|Research & Develo...|Laboratory Techni...| 34|  Male|  2661|     3|    9980|\n",
      "|        15|Research & Develo...|Laboratory Techni...| 28|  Male|  2028|     6|    3298|\n",
      "|        16|Research & Develo...|Manufacturing Dir...| 29|Female|  9980|    10|    2935|\n",
      "|        17|Research & Develo...|  Research Scientist| 32|  Male|  3298|     7|   15427|\n",
      "|        18|Research & Develo...|Laboratory Techni...| 22|  Male|  2935|     1|    3944|\n",
      "|        19|               Sales|             Manager| 53|Female| 15427|    31|    4011|\n",
      "|        20|Research & Develo...|  Research Scientist| 38|  Male|  3944|     6|    3407|\n",
      "+----------+--------------------+--------------------+---+------+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT EmployeeID,Department,jobRole,age,gender,Income,Workex,\n",
    "        lead(Income, 2, 0) OVER(ORDER BY EmployeeID) next_inc\n",
    "        FROM hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81955c9e",
   "metadata": {},
   "source": [
    "#### NTILE() \n",
    "        * Dividing records into number of Quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85762cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "|EmployeeID|          Department|             jobRole|age|gender|Income|salary_quartiles|\n",
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "|       514|Research & Develo...|  Research Scientist| 20|  Male|  1009|               1|\n",
      "|       728|Research & Develo...|  Research Scientist| 18|  Male|  1051|               1|\n",
      "|       765|               Sales|Sales Representative| 28|  Male|  1052|               1|\n",
      "|      1338|               Sales|Sales Representative| 30|  Male|  1081|               1|\n",
      "|      1365|               Sales|Sales Representative| 29|  Male|  1091|               1|\n",
      "|       178|Research & Develo...|Laboratory Techni...| 19|  Male|  1102|               1|\n",
      "|       912|               Sales|Sales Representative| 25|  Male|  1118|               1|\n",
      "|      1402|Research & Develo...|Laboratory Techni...| 31|Female|  1129|               1|\n",
      "|       302|               Sales|Sales Representative| 18|Female|  1200|               1|\n",
      "|       911|Research & Develo...|  Research Scientist| 23|  Male|  1223|               1|\n",
      "|        24|Research & Develo...|  Research Scientist| 21|  Male|  1232|               1|\n",
      "|      1017|Research & Develo...|  Research Scientist| 31|Female|  1261|               1|\n",
      "|      1053|Research & Develo...|  Research Scientist| 30|  Male|  1274|               1|\n",
      "|       516|Research & Develo...|Laboratory Techni...| 35|  Male|  1281|               1|\n",
      "|      1013|               Sales|Sales Representative| 31|Female|  1359|               1|\n",
      "|      1205|Research & Develo...|Laboratory Techni...| 32|  Male|  1393|               1|\n",
      "|       778|Research & Develo...|Laboratory Techni...| 21|Female|  1416|               1|\n",
      "|       297|Research & Develo...|Laboratory Techni...| 18|  Male|  1420|               1|\n",
      "|       150|Research & Develo...|Laboratory Techni...| 19|Female|  1483|               1|\n",
      "|      1311|Research & Develo...|  Research Scientist| 18|Female|  1514|               1|\n",
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT EmployeeID,Department,jobRole,age,gender,Income,\n",
    "        NTILE(4) OVER(ORDER BY income) salary_quartiles\n",
    "        FROM hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3bbdd",
   "metadata": {},
   "source": [
    "#### Find the number of Employee in each percentile_group 0-25 ,25-50,50-75 based on Income using percent_rank and case when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59e4c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|percentile_range|Emp_Count|\n",
      "+----------------+---------+\n",
      "|            0-25|      367|\n",
      "|           25-50|      366|\n",
      "|           50-75|      367|\n",
      "|          75-100|      369|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT percentile_range, count(*) Emp_Count\n",
    "FROM (\n",
    "    SELECT\n",
    "        INCOME,\n",
    "        PERCENT_RANK() OVER(PARTITION BY DEPARTMENT ORDER BY INCOME) * 100 as percent_rank,\n",
    "        CASE \n",
    "            WHEN PERCENT_RANK() OVER(PARTITION BY DEPARTMENT ORDER BY INCOME) * 100 < 25 THEN '0-25'\n",
    "            WHEN PERCENT_RANK() OVER(PARTITION BY DEPARTMENT ORDER BY INCOME) * 100 < 50 THEN '25-50'\n",
    "            WHEN PERCENT_RANK() OVER(PARTITION BY DEPARTMENT ORDER BY INCOME) * 100 < 75 THEN '50-75'\n",
    "            ELSE '75-100'\n",
    "        END as percentile_range\n",
    "    FROM hremployee\n",
    "    ) _ \n",
    "GROUP BY percentile_range\n",
    "ORDER BY percentile_range\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b246d2c",
   "metadata": {},
   "source": [
    "#### Hive Integration with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcc841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d04e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2482 NameNode\n",
      "3268 NodeManager\n",
      "3093 ResourceManager\n",
      "12327 SparkSubmit\n",
      "12455 Jps\n",
      "2683 DataNode\n",
      "2924 SecondaryNameNode\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998e3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Integration  with Hive warehouse.\n",
    "# Config name for Hive-Integration 'spark.sql.warhouse.dir' value = '/user/hive/warehouse'\n",
    "spark = (SparkSession.builder.appName('Pyspark-Hive')\n",
    "         .config('spark.sql.warehouse.dir','/user/hive/warehouse')\n",
    "        .enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aadafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark-Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff4c2cad198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefe0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    airlines|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1615df77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"\"\"\n",
    "     CREATE DATABASE IF NOT EXISTS airlines\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2442725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     use airlines\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b70a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    airlines|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     show databases\n",
    " \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e44ed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    show tables\n",
    " \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab74fe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     create table if not exists flights(\n",
    "             DayOfMonth int, \n",
    "             DayOfWeeK int,\n",
    "             Carrier varchar(10),\n",
    "             OriginAirportID int,\n",
    "             DestAirportID int,\n",
    "             DepDelay int,\n",
    "             ArrDelay int\n",
    "             )\n",
    "             row format delimited\n",
    "             fields terminated by ','\n",
    "             lines terminated by '\\n'\n",
    "             STORED AS TEXTFILE\n",
    "             TBLPROPERTIES('skip.header.line.count' = '1')\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942b7d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dee377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"load data local inpath '/home/hadoop/Downloads/raw_flight_data1.csv' overwrite into table flights\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d21331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6503cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table if not exists airports(airport_id int, city varchar(50),state varchar(50) )\n",
    "    row format delimited\n",
    "    fields terminated by ','\n",
    "    lines terminated by '\\n'\n",
    "    stored as textfile\n",
    "    tblproperties ('skip.header.line.count' = '1')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71fd0861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"load data local inpath '/home/hadoop/Downloads/airports1.csv' overwrite into table airports\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2aa8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f0d912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayOfMonth|DayOfWeeK|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|     -19|\n",
      "|        19|        5|     DL|          15016|        10397|       0|      -1|\n",
      "|        19|        5|     DL|          10397|        14869|      15|      24|\n",
      "|        19|        5|     DL|          10397|        10423|      33|      34|\n",
      "|        19|        5|     DL|          11278|        10397|     323|     322|\n",
      "|        19|        5|     DL|          14107|        13487|      -7|     -13|\n",
      "|        19|        5|     DL|          11433|        11298|      22|      41|\n",
      "|        19|        5|     DL|          11298|        11433|      40|      20|\n",
      "|        19|        5|     DL|          11433|        12892|      -2|      -7|\n",
      "|        19|        5|     DL|          10397|        12451|      71|      75|\n",
      "|        19|        5|     DL|          12451|        10397|      75|      57|\n",
      "|        19|        5|     DL|          12953|        10397|      -1|      10|\n",
      "|        19|        5|     DL|          11433|        12953|      -3|     -10|\n",
      "|        19|        5|     DL|          10397|        14771|      31|      38|\n",
      "|        19|        5|     DL|          13204|        10397|       8|      25|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from flights').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9aa14",
   "metadata": {},
   "source": [
    "### 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea478478",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.table(\"airlines.flights\")\n",
    "airports_df = spark.table(\"airlines.airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88232436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+\n",
      "|airport_id|       city|state|\n",
      "+----------+-----------+-----+\n",
      "|     10165|Adak Island|   AK|\n",
      "|     10299|  Anchorage|   AK|\n",
      "+----------+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b495892",
   "metadata": {},
   "source": [
    "### 2. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4051a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_df.join(airports_df, on= flights_df.OriginAirportID == airports_df.airport_id , how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa566f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+\n",
      "|DayOfMonth|DayOfWeeK|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|airport_id|          city|state|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|     11433|       Detroit|   MI|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|     14869|Salt Lake City|   UT|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|     14057|      Portland|   OR|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|     15016|     St. Louis|   MO|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|     11193|    Cincinnati|   OH|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13f058",
   "metadata": {},
   "source": [
    "### 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc94bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_join.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e5dad86",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/hadoop/Downloads/flights already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o126.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/hadoop/Downloads/flights already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-79875826502a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///home/hadoop/Downloads/flights/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/hadoop/Downloads/flights already exists.;'"
     ]
    }
   ],
   "source": [
    "flights_df.write.parquet(\"file:///home/hadoop/Downloads/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92bfe32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a parquet file format\n",
    "flights_parquet_df = spark.read.parquet(\"file:///home/hadoop/Downloads/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247984ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.parquet(\"/flights1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy('Carrier').parquet('/airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f617b9fc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table `bucketed_table` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o175.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Table `bucketed_table` already exists.;\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:444)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1bdcd660cf3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights_join\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucketBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumBuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bucketed_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table `bucketed_table` already exists.;'"
     ]
    }
   ],
   "source": [
    "flights_join.write.bucketBy(col = 'state', numBuckets = 50).format(\"csv\").saveAsTable(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c3f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f0e08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy('Carrier').bucketBy(col = 'state', numBuckets = 30)\\\n",
    ".format(\"csv\").saveAsTable(\"Part_bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c7cd59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|carrier|count(1)|\n",
      "+-------+--------+\n",
      "|     UA|  122443|\n",
      "|     AA|  124037|\n",
      "|     EV|   46563|\n",
      "|     B6|   51381|\n",
      "|     DL|  134724|\n",
      "|     OO|   69785|\n",
      "|     F9|    9811|\n",
      "|     YV|   14612|\n",
      "|     US|  100668|\n",
      "|     MQ|   45926|\n",
      "|     HA|    4962|\n",
      "|     AS|   28796|\n",
      "|     FL|   28053|\n",
      "|     VX|   14683|\n",
      "|     WN|  216101|\n",
      "|     9E|   36030|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT carrier, count(*) from part_bucketed_table group by carrier\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db791d",
   "metadata": {},
   "source": [
    "### Load on MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea228472",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_properties = {\n",
    "    'user' : 'root',\n",
    "    'password' : 'hadoop@123',\n",
    "    'driver' : 'com.mysql.cj.jdbc.Driver'\n",
    "}\n",
    "\n",
    "flights_join.write.jdbc(url=\"jdbc:mysql://localhost:3306/flights\",table='airlines',\n",
    "                        mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf40d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079da45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
